\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Trex discorso ― versione 0.1 ― powered by imprecazione}
\author{Daniele per Tracking Exposed }
\date{October 2020}

\begin{document}

\maketitle

\section{Introduzione}

# Internet e Big Data

La connessione ininterrotta ad internet che sperimentiamo in questi anni è una novità, la rete è un formidabile strumento di raccolta dati;
scollegarsi non è un'opzione praticabile, a meno di rinunciare a quasi ogni servizio possibile, dalla relazioni con le proprie cerchie, al
noleggio di un'automobile o la gestione di un conto in banca. Inoltre la pandemia ha imposto di recente l'introduzione del distanziamento
sociale, aumentando di frequenza e importanza le pratiche di connessione remota come la teledidattica, il telelavoro e gli acquisti online.

Ad ogni connessione gli individui lasciano dietro di sé un grande numero di dati e metadati, che vengono utilizzati come merce dalle ditte
private che li raccolgono tramite servizi "gratuiti", come ad esempio i social network. Questi dati sempre aggiornati, aggregati in grandi
quantità, vengono chiamati "Big Data" e sono commerciabili in quanto di grande utilità per disegnare il carattere e le abitudini degli
individui; vengono utilizzati non solo per individuare e catalogare gusti e opinioni, ma anche per stimolarli. Possono infatti indirizzare
l'opinione a scopo pubblicitario ma anche influenzare il comportamento sessuale e creare consenso per la governance.

La capacità di raccogliere e processare grandi quantità di dati è privilegio delle entità che possiedono i mezzi di produzione adatti a
farlo. Queste entità non corrispondono alle istituzioni locali o statali, ma sono ditte private come ad esempio Google, Amazon e
Facebook. Le quali li accumulano, li trattano, li usano e a volte li rivendono in un'ottica di profitto privato che non per forza
corrisponde a un bene comune. Ci troviamo di fronte ad una asimmetria informativa e dunque di potere. Le leggi sulla Privacy non possono che
rincorrere e non possono certamente precedere l'innovazione scientifica e sono dunque inadeguate a rapportarsi con il potere delle entità
private, spesso extra nazionali, che hanno a disposizione i dati dei cittadini.

# Algoritmica

Il modo in cui i Big Data vengono raccolti e trattati, le modalità di analisi dei dati, chiama in causa la parola Algoritmo. Gli algoritmi
sono calcoli finiti che hanno lo scopo di trarre conclusioni, come una ricetta. Sappiamo che la Ricetta non è neutrale, come non lo è la
Statistica e non lo è l'Algoritmo. Il punto d'osservazione, le modalità di acquisizione, la capacità e la volontà di cercare o dimostrare
qualcosa influiranno sul risultato finale, che verrà poi fornito formattato per ulteriore interpretazione. Il modo in cui viene presentata
un'informazione contiene già un giudizio, un punto di vista e un obiettivo.

Gli algoritmi che vengono utilizzati per raccogliere e utilizzare i dati dalle ditte che offrono servizi online non sono pubblici, il loro
funzionamento è un segreto aziendale. Non sappiamo con quale criterio dopo un determinato video su Youtube ce ne viene proposto un altro,
possiamo intravvedere lo scopo pubblicitario, ma non quello politico. Non sappiamo ancora come funziona, ma ci siamo accorti come queste
informazioni siano utilizzate per determinare il consenso e per regolare le comunità al suo interno.

# Influenza politica

Il caso Cambridge Analytica è significativo. Nel 2018, grazie a una inchiesta giornalistica del canale Tv inglese Channel 4, viene rivelato
che la società UK Cambridge Analytica ha usato a scopo politico i dati degli utenti, fornendo loro informazioni costruite e mirate, azione
che nel loro gergo tecnico viene detta segmentazione, allo scopo di influenzare le scelte elettorali. La società ha ricevuto i dati da
Facebook, ufficialmente senza pagarli direttamente ma figurando come ricercatori. Negli scorsi anni ha influenzato le elezioni di Argentina,
Nigeria, Italia e Stati Uniti. Per stessa ammissione del responsabile Alexander Nix, [registrata in video][], le attività di gestione del
consenso non si limitavano all'uso dei dati, ma all'occasione potevano comprendere attività più palesemente illecite, come l'utilizzo di sex
workers per ricattare i candidati politici scomodi. Lo scandalo travolge Cambridge Analytica, chiudendola, ma anche Facebook. Quando il suo
fondatore Mark Zuckerberg sarà interrogato al Senato USA, alla domanda su quale sia la natura dei suoi affari risponderà: "Senator, we run
ads - Senatore, noi facciamo pubblicità". Fino ad allora Facebook non si riferiva a sé come a un'agenzia pubblicitaria, ma come a un
servizio gratuito per mettere in collegamento le persone nel mondo. Il suo potere è però da allora pubblicamente noto: Facebook ha la
possibilità di influenzare la politica.

[registrata in video]:https://www.channel4.com/news/cambridge-analytica-revealed-trumps-election-consultants-filmed-saying-they-use-bribes-and-sex-workers-to-entrap-politicians-investigation

# Tracking Exposed

Tracking Exposed nasce nel 2016, per analizzare dall'esterno il funzionamento di un algoritmo.

Per andare verso un'etica dell'informazione, dobbiamo essere in grado di interpretare il funzionamento degli algoritmi che trattano le
informazioni che ci riguardano. Non avendo accesso al codice sorgente degli algoritmi, possiamo applicare il metodo scientifico creando una
situazione mirata, raccogliere i dati che ne scaturiscono e confrontarli. Sì, stiamo tirando palline da ping pong verso un muro invisibile e
cerchiamo di capire la natura del muro studiando le palline che ci ritornano.

Una delle funzioni degli algoritmi è di tracciare i comportamenti personali e fornire risposte diverse a persone diverse, ad esempio in
quale diverso ordine vengono proposti gli articoli sulla timeline di Facebook. Per questo abbiamo creato un componente aggiuntivo per il
navigatore in modo che su base volontaria le persone possano fornire i dati che Facebook crea durante l'esperienza di utilizzo e rilevare le
diversità di trattamento tramite comparazione.

Lo scopo di Tracking Exposed è rivelare la natura ed il funzionamento delle tecnologie di tracciamento, dato che le conseguenze sul mondo
reale sono importanti. È un compito non risolvibile con un approccio puramente legislativo e certamente è vano chiedere alla stessa entità
che trae profitto dall'uso dell'algoritmo che ha essa stessa progettato come segreto industriale, di spiegare e rendere pubblico il suo
funzionamento. Ancor più illusorio è credere che l'azienda stessa possa e voglia porre un rimedio per mitigare le conseguenze nocive dei
suoi algoritmi, per evidente contrasto con il proprio interesse privato. Le richieste periodicamente provenienti dalle istituzioni nazionali
nei confronti delle grandi piattaforme internazionali, mancando della capacità esecutiva, finiscono per somigliare a lamenti.

# Approccio legislativo

***QUI

Ogni tentativo di interazione di tipo legislativo si è rivelato fino ad oggi insufficiente, ma di fronte all'obsolescenza delle leggi - il
legislatore segue l'innovazione tecnologica, non può precederla - l'approccio non può essere puramente di tipo legislativo, inoltre
l'aumentare dell'automazione nella burocrazia di ambito anche giuridico, l'uso delle intelligenze artificiali in ambito normativo e la
tendenza a costruire regole basandosi sui dati *data driven policy* rende urgente avere la possibilità di conoscere e poter decostruire gli
elementi che partecipano alla formazione dei dati. Occorre incamminarsi verso un tipo di etica dell'informazione che consideri l'individuo
come agente libero e responsabile, portatore di un pacchetto di informazioni di cui va salvaguardata l'integrità, non solo la proprietà, e
che sia questo approccio etico a guidarci nelle scelte future. Occorre che le istituzioni di monitoraggio etico-politico *watchguard* siano
in grado di effettuare le analisi in proprio e in maniera indipendente, sia possedendo gli strumenti tecnici che il metodo di
analisi. Perché siano in grado di collaborare tra loro e con la comunità scientifica.

Sappiamo che la tecnologia non è neutrale, che l'intelligenza artificiale fallisce nel comprendere contesto e intento e che l'algoritmo è oggi strumento di governance politica. Un discorso di decostruzione del potere ha bisogno di farsi comprendere, per questo si avrà un approccio interdisciplinare. Per comprendere e farci comprendere chiederemo aiuto alla sociologia, alla semiotica, alla psicologia ma prima ancora crediamo che servano evidenze tecnologiche ricavate empiricamente: nasce tracking exposed.

Gli algoritmi organizzano il tempo e il consenso. Non esiste una soluzione tecnica (tecno-soluzionismo) perché Il problema non è solo tecnico, ma sociale, giuridico, politico e tecnico. La rete non è uno spazio staccato dalla realtà, le conseguenze della repressione algoritmica variano a seconda della situazione geopolitica dove si esprimono, ma ricadono sempre e con grande violenza sul corpo. Ad esempio, un post "sbagliato" su Facebook in Italia può causare un licenziamento, sempre su Facebook ma in un Paese in guerra può costare la vita.

Crediamo che ai Big Data si debba rispondere con Big Rights. È una questione politica e non solo economica. Per avere dei diritti bisogna esercitarli e dunque conoscerli. I dati possono essere utilizzati, in un contesto democratico, nell'interesse collettivo (common good), in attesa che questo sia possibile, vogliamo contribuire a tenere aggiornata la pubblica opinione e la comunità scientifica sulle dinamiche dello sfruttamento dati e stimolare un dibattito a questo proposito. Lo scopo di Tracking Exposed è di evidenziare e rendere esercitabili i diritti legati allo sfruttamento dei dati, fare informazione e sviluppare critica, allo stato attuale non possiamo ambire a promuovere soluzioni, ma a far conoscere il funzionamento degli algoritmi proprietari, il quale è un requisito necessario per ogni forma di democrazia moderna, perché Internet può diventare un mezzo di oppressione. La causa della libertà nel secolo 21 è inestricabilmente connessa alla resistenza alla sorveglianza elettronica.

\subsection{Alcune posizioni citabili:}

"Non si può mangiare un confetto pretendendo di sentire – solo perché si ha una vasta cultura e un forte controllo delle proprie sensazioni – sapore di sale. La chimica non sbaglia mai. Siccome esiste anche una chimica delle emozioni, e uno dei composti che per antica tradizione suscitano emozioni è un intreccio ben congegnato, se un intreccio è ben congegnato suscita le emozioni che si era prefisso quale effetto. Potremo poi, après coup, criticarci per averle provate, o criticarle come emozioni repellenti, o criticare le intenzioni con cui è stata congegnata la macchina che le ha provocate. Ma questa è un'altra storia. Un intreccio ben temperato produce gioia, terrore, pietà, riso o pianto."
Umberto Eco, Il superuomo di massa, 1976

"Quando la signora Thatcher è stata eletta per la seconda volta, aveva ingaggiato la Saatchi & Saatchi, una grossa compagnia pubblicitaria, per la sua campagna. E i pubblicitari hanno utilizzato tutti i possibili trucchi, dai giri di frase calcolati per suscitare facili emozioni, ai colori dei suoi vestiti o delle tende davanti alle quali si metteva, fino al calcolo preciso di quando comparire e scomparire dai media. E intanto la sua nobile opposizione socialista disprezzava quei trucchi e i media. Abbiamo potuto osservare bene l'attenta regia della campagna della signora Thatcher, seguendo un geniale programma televisivo. Quando dico "noi" intendo quella minoranza del paese che l'ha seguito, mentre ritengo che guardarlo avrebbe dovuto essere obbligatorio. Siamo arrivati a un punto in cui un leader politico non solo usa abilmente i vecchi trucchi teatrali per fomentare la folla, come faceva Giulio cesare nel dramma di Shakespeare, ma ingaggia gli esperti che rendono tutti quei trucchi più efficaci. L'antidoto però consiste nel fatto che in una società aperta possiamo anche esaminare quei trucchi mentre vengono usati con noi. Naturalmente sempre che scegliamo di esaminarli, sempre che non cambiamo canale per vedere Dallas o qualsiasi altra cosa."
Doris Lessing, le Prigioni che abbiamo dentro, 1987

"Mentre le merci sono separate da noi e possono essere portate a casa per essere analizzate prima di essere consumate, l'informazione entra a far parte di noi non appena viene acquistata, e provoca in noi cambiamenti determinanti, se considerati nella prospettiva dell'intellettualismo etico (314b), per la quale l'ignoranza e la disinformazione sono la differenza decisiva fra il male e il bene".
Il [Protagora][] di Platone di Maria Chiara Pievatolo [Protagora]: https://btfp.sp.unipi.it/dida/protagora/ar01s02.xhtml#mercimathema

"Libertà non sta nello scegliere tra bianco e nero, ma nel sottrarsi a questa scelta prescritta". Theodor Adorno

"Conoscere gli individui meglio di quanto conoscano sé stessi è un formidabile mezzo di controllo". Edward Herman e Noam Chomsky, Manufacturing consent, 1988

"L'individuo in rete è rappresentato dall'insieme dei sui dati". Stefano Rodotà

"Internet non è più uno spazio libero e indipendente, ma è commercialmente controllato e personalizzato, Google e Facebook ciberanno gli utenti di ciò che vogliono che veda, il computer è diventato uno specchio che riflette i tuoi interessi e rinforza i tuoi pregiudizi". Eli Pariser, Filter bubble, 2011.

\section{Gli algoritmi di personalizzazione}

Gli algoritmi di personalizzazione sono nati perché le compagnie volevano rendere l'esperienza online meno noiosa, piú efficiente e soddisfacente. Gli algoritmi sono serviti ad evitare lo spam. Oggi regolano il discorso pubblico all'interno dei social media e di conseguenza sulla governance, fino all'individuo. Disincentivano il pensiero critico: se radio e la tv sono strumenti di manipolazione di massa, amalgamatore di immaginari ed unificatore linguistico, almeno almeno si può dire dell'internet della raccolta dati. Che sia per limitare l'information overload, o che sia per tenere l'utente incollato allo schermo e vendere più pubblicità, l'algoritmo svolge una funzione di gatekeeping non solo automatizzato, ma i cui presupposti ideologicizzati non sono accessibili.

Data la loro complessità, gli algoritmi non sono comprensibili dal pubblico e inoltre sono segreti, le compagnie non cedono i dati e tantomeno le formule. È irrealistico chiedere alle compagnie di autoregolarsi. Per verificare che le imprese non mettano in atto pratiche scorrette nei confronti del consumatore si rivela necessaria un'analisi terza ed indipendente sugli algoritmi che possono condizionare indebitamente. Inoltre non si può discutere di vessatorietà di clausole contrattuali senza considerare come effettivamente i dati del consumatore vengono sfruttati per discriminare, non solo sulla base di nazionalità e luogo di residenza, ma anche sulla base di dati molto più sensibili quali orientamento sessuale, reddito e disponibilità economica, stato civile e livello di istruzione. Le dichiarazioni delle imprese non sono sufficienti. È data la possibilità di documentare e analizzare prove circa l'aggressività e la pervasività di pratiche commerciali dai tempi e modalità personalizzate algoritmicamente. Gli algoritmi devono essere conosciuti attraverso l'analisi esterna e indipendente.

Le conseguenze della repressione algoritmica sono in parte ancora da vedere, ma sappiamo già che: "Conoscere gli individui meglio di quanto conoscano sé stessi è un formidabile mezzo di controllo." Manufacturing consent, Edward Herman e Noam Chomsky, 1988. Sappiamo anche che "Internet non è più uno spazio libero e indipendente, ma è commercialmente controllato e personalizzato, Google e Facebook ciberanno gli utenti di ciò che vogliono che veda, il computer è diventato uno specchio che riflette i tuoi interessi e rinforza i tuoi pregiudizi." Filter bubble, Eli Pariser, 2011.

\subsection{Intermezzo: l'Algoritmo Captivo}

\emph{
Qui mi domando se partire con una cosa che per quanto mi piaccia, potrebbe non c'entrare nel contesto di questo documento, ma potrebbe essere un articolo a parte. Mi riferisco alla considerazione della parola "cattivo" nel suo significato originale di "captivo", ossia prigioniero. E costruire un simpatico pensierino sul fatto che se non viene "liberato", come può essere vista l'azione di Trex nello svelarne il funzionamento, l'algoritmo, prigioniero nei vincoli tecnologici e legali che tengono oscuro il suo funzionamento, fa danno sociale. Dunque occorre liberarlo. (liberare l'algoritmo, per liberarci). Mi sembra carina, ma magari è più che altro uno spunto narrativo per scrivere un articolo su Trex, me la segno per non scordarla e anche per sapere la vostra.}

\section{Trex: Tracking Exposed}

Siamo un progetto no-profit che usa software libero per analizzare prove di personalizzazione algoritmica. Permettiamo agli utenti dei social media di setacciare e raccogliere i dati che gli vengono imboccati e di analizzarli per comparazione, offrendo loro interfacce rispettose della privacy. Allo scopo di rivelare quanto sia aggressivo e manipolatorio il moderno panorama di Internet e le sue conseguenze.

Trex investiga gli algoritmi. Riportiamo il potere di analisi e controllo dei dati agli utenti che i dati producono e dovrebbero dunque possedere. Forniamo alle persone gli strumenti per controllare il controllore. Crediamo che per attenuare la repressione algoritmica, da parte di chi dispone del potere di raccoglierli e utilizzarli, sia necessario che il funzionamento degli algoritmi sia trasparente al pubblico. L'azione di Trex è di incisione sulla realtà, rivelando il funzionamento dei sistemi di profilazione personalizzati e producendo prove del loro funzionamento. Lo scopo di Trex è abilitare le persone, le quali vengono chiamate: "utenti" nel loro rapporto con la tecnologia di consumo, a mantenere la loro parte di potere che è rappresentata dai loro dati.

Per fare tecnologia, ossia per programmare strumenti di analisi aggiornati e per fare educazione, divulgare gli strumenti e i metodi di lavoro sia per altri analisti, che in ambito di ricerca, abbiamo bisogno di una copertura economica che ci permetta di non basarci solamente sul volontariato, soprattutto considerando la statura materiale delle entità commerciali che confrontiamo. Crediamo che il momento storico sia paragonabile al medioevo (nella comune e non del tutto esatta accezione di epoca oscura e superstiziosa) per quanto riguarda gli strumenti digitali, l'uso della rete e il suo impatto sulla società è in corso e crediamo che il nostro lavoro abbia un'importanza considerevole, che pensiamo aumenterà d'importanza col tempo.

"La libertà non può essere gratis". Seneca

\section{Metodo}

Tracking Exposed utilizza il metodo scientifico. Fa uso di tecnologie aperte e verificabili, rende pubblici e così utilizzabili dal pubblico e dalla comunità scientifica i risultati delle analisi e gli strumenti utilizzati, la metodologia, il contesto e i dati raccolti nella loro interezza perché siano sottoposti a ulteriore verifica. Accompagniamo le analisi a una descrizione trasparente per permettere replica e confutazione. Costruiamo in proprio gli strumenti di analisi quando questi non sono già esistenti e li rendiamo disponibili, scaricabili e utilizzabili liberamente senza alcun bisogno di autorizzazione da parte nostra. Siamo indipendenti nella raccolta dei dati, nella scelta di quale obiettivi perseguire, della posizione di osservazione adottata e dei finanziamenti ricevuti rispetto alle dinamiche algoritmiche osservate (vedi: la Storia di Trex). Tendiamo a un approccio interdisciplinare per tenere conto della complessità dell'osservazione e per mantenere la capacità di comunicare i risultati, ma aderiamo al metodo sperimentale di raccolta prove e analisi comparativa.

Utilizziamo lo schema scientifico di riproducibilità sull'apprendimento automatico, versione 2,7 Aprile 2020 Reproducibility checklist: Per ogni modello e algoritmo presentato, viene inclusa una descrizione chiara degli assunti matematici e dei modelli considerati, analisi della complessità che comprende: tempo, spazio e quantità di dati per ogni algoritmo. Ogni ipotesi teorica comprende la prova in completezza e la sua descrizione. Ogni banca dati utilizzata è accompagnata dalla statistica di rilievo, come numero di esempi, andamento, validazione e diversità dei test effettuati; quali dati sono stati esclusi e perché, assieme ai passi preliminari. Pubblichiamo la versione scaricabile dei dati e dell'ambiente di simulazione, ogni dato raccolto è accompagnato da una descrizione completa del processo di collezione, incluse le istruzioni e i metodi per la verifica. I codici condivisi comprendono le specifiche delle dipendenze, l'evoluzione del codice, preparazione, modelli e documento LEGGIMI con i comandi da eseguire per riprodurre i risultati.

Il nostro scopo è non solo di fornire un metodo utilizzabile in ambito di ricerca o di studio, ma di parallelamente permettere all'utilizzatore finale di compiere le sue proprie ricerche ed analisi, in quanto pensiamo che la capacità di analisi dei dati sia da riportare alle persone cui i dati appartengono. Le persone usando la rete dovrebbero avere persino la possibilità di comporre il loro, proprio algoritmo. Abbiamo scoperto che le nostre analisi hanno l'utilità di decostruire l'azione delle piattaforme e in alcuni casi di permettere l'individuazione di campagne di disinformazione. Riconosciamo tre categorie di attori sociali che chiamiamo: utenti, produttori di news e piattaforme. Dividiamo tra dieta informativa e diversità informativa all'interno delle analisi di quella che riconosciamo come filter bubble. Infine, abbiamo sperimentato l'opposizione delle piattaforme alle analisi che le riguardano, non meramente sul piano legale, ma soprattutto su quello più materiale dell'opacità, assenza delle informazioni - il rilascio dati è sempre incompleto - e addirittura su quello tecnocratico del renderle esplicitamente inaccessibili. L'analisi delle API non è sufficiente e inoltre le compagnie aggiornano e chiudono le API quando vogliono, godendo della evidente asimmetria di potere. Anche per questo stiamo transitando da una metodologia di analisi quantitativa verso un'analisi qualitativa e di inserimento di ulteriori elementi contestuali.

Tracking Exposed rappresenta un punto d'osservazione indipendente e si propone di rendere visibile il tracciamento, cioè il modo in cui i social network studiano le persone, raccolgono e processano i loro dati secondo attività e interazioni per arrivare a proporre contenuti mirati. La sorveglianza non è visibile per le persone, perché gli strumenti di analisi (algoritmi) sono oscuri. Questo produce discriminazione algoritmica. Nell'intento di rendere le nostre analisi comprensibili e utilizzabili nel dibattito contemporaneo, tentiamo di non produrre solo dati percentuali, ma analisi che, partendo da dati ricavati con metodo scientifico esperienziale (sperimentale), producano elementi di critica e di utilità sociale.

La base del nostro metodo consiste nel cercare evidenze nel diverso trattamento automatizzato che ricevono due utenti simili. Per ricavare i dati di utenti simili mentre accedono allo stesso servizio, abbiamo adottato sia la tecnica di creare utenti appositamente, sia quella di chiedere su base volontaria di fornirci i dati. La comparazione tra i dati raccolti da un browser pulito, senza tracce di navigazione precedente e uno già utilizzato, dunque contenente tracce personalizzate, offre per la nostra analisi i risultati più interessanti.

Descriviamo in modo accurato il metodo con il quale effettuiamo il test; sia per lasciare a chi legge gli strumenti necessari per poterlo replicare, che per avere la possibilità di mettere in discussione le inevitabili mancanze che ogni test contiene.

Per raccogliere i dati del trattamento che ricevono gli utenti facendone una copia, abbiamo costruito un'estensione del browser che colleziona quello appare all'utente.

Rilasciamo i dati dei nostri esperimenti per permettere ad altri gruppi di ricerca di verificare o confutare le nostre analisi, e perché possano usare lo strumento. L'algoritmo continua a cambiare, il nostro software è appositamente rilasciato sotto licenza libera perchè sia utilizzabile, solo collettivamente possiamo interrogarci su quali sistemi debbano regolare i nostri discorsi online, l'alternativa, sarebbe tecnocrazia.

Abbiamo scoperto che l'acquisizione dei dati è il momento più sensibile (delicato?).

Nella ricerca di nuove metriche per capire e misurare l'algoritmo, abbiamo iniziato ad usare l'espressione: Dieta informativa, Per dare un nome al regime cui Facebook sottopone i suoi utenti. A prescindere dai valori utilizzati da Facebook, il nostro scopo è evidenziare ed eventualmente trasformare l'arbitrarietà con la quale l'algoritmo decide per nostro conto.

Abbiamo incontrato, durante questo esperimento, anche il concetto di Diversità informativa. Ossia la probabilità con la quale l'algoritmo proponga contenti che l'utente non hai mai visto prima. Possiamo dedurre che l'esposizione a contenuti diversi o la riproposizione di contenuti già visti impatti direttamente sulla diversità di opinioni al quale l'utente viene esposto e di conseguenza nella percezione della pluralità. Abbiamo chiamato questa variabile di proposta di contenuti sempre diversi Diversità informativa.

Per meglio analizzare la filter bubble, abbiamo osservato come i giornali online vengono trattati da Facebook, constatando che l'algoritmo non considera solamente le preferenze, cioé i like. Come neppure le risorse investite dalle attività commerciali, che avrebbero dato il ripetersi delle stesse proporzioni di proposte agli utenti. Sembra che il giornale più avvantaggiato sia quello centrista, come se l'algoritmo penalizzasse le posizioni più radicali, e nel verificare che le pagine del giornale più seguito sono proposte più spesso, si potrebbe dedurre che l'algoritmo preserva lo status quo, ma purtroppo non abbiamo ancora una chiara determinazione; la metodologia serve a verificare ipotesi che poi impattano sulle nostre diete informative individuali.

Va fatto notare che Facebook attivamente ostacola analisi indipendenti. E questo non avviene tramite pressioni legali, come nel caso di Spotify. La resistenza di Facebook usa il suo strapotere tecnologico. Rendendo difficile la creazione di utenti sperimentali, individuando attività che si discostano dal comune, o nascondendo nel loro codice HTML porzioni atte solo ad offuscare i dati che ci servono per i raffronti. La protezione dell'algoritmo non avviene con meccanismi legali, ma tecnologici.

Ad oggi non sappiamo ancora giudicare gli algoritmi ed il loro potere. Analisti come noi stanno inventando tassonomie e le mettono a verifica, ma quando la società civile pone delle richieste alle piattaforme, queste sono sempre volte a risolvere un problema di tipo politico, come il rimuovere la disinformazione o i messaggi d'odio, o evitare il bullismo online. Pensiamo che da una parte non è accettabile che si deleghi una responsabilità censoria ad un'azienda privata, e inoltre che non vada accettata questa tendenza a delegare lo spirito critico. Le dinamiche di potere delle reti vanno affrontate decostruendo e scorporando questi poteri. Dobbiamo poter usare i dati in modo costruttivo.

\section{Proposta}

Tracking exposed offre strumenti, esperienze, metodo e discorso sull'analisi degli algoritmi. Dall'analisi passiva delle esperienze personalizzate, fino alla declinazione degli effetti sugli esperimenti effettuati.

Fondamentale la comprensione che qualunque risposta al funzionamento di un algoritmo non può avere una risposta soddisfacente da parte della stessa entità che questo algoritmo ha inventato ed utilizza, per evidente conflitto interessi. Non è pensabile che sia la stessa ditta che produce l'algoritmo ad autoregolamentarsi, inoltre l'algoritmo é cosí personalizzato, geo-localizzato e regionalizzato per cui la versione che gira in un luogo, non vale per l'altro.

Le competenze e le esperienze pratiche di trex sono condivisibili: utilizzabili da ricercatori come metodo di analisi o da un'entità produttrice di contenuti che voglia capire le logiche di valutazione e di un partner che voglia supportare l'esperienza e il lavoro di Trex.

Offriamo test affidabili, in un panorama dove anche i test inaffidabili vengono comunque ripresi e utilizzati e i racconti aneddotici rischiano di essere il riferimento più ricorrente.

\section{Storia di Tracking Exposed}
\section{Altre sezioni?}

\end{document}
